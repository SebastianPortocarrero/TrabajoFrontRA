# ğŸ¤– ConfiguraciÃ³n de IA Local - Ollama + Llama 3

## ğŸš€ **Paso 1: Instalar Ollama**

### Windows/Mac:
1. Ve a https://ollama.com/
2. Descarga e instala Ollama
3. Una vez instalado, abre una terminal

### Linux:
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

## ğŸ§  **Paso 2: Descargar Llama 3**

Ejecuta en tu terminal:

```bash
# OpciÃ³n 1: Llama 3.2 (mÃ¡s rÃ¡pido, 3B parÃ¡metros)
ollama pull llama3.2

# OpciÃ³n 2: Llama 3.1 (mÃ¡s potente, 8B parÃ¡metros)
ollama pull llama3.1:8b

# OpciÃ³n 3: Si tienes mucha RAM (70B parÃ¡metros)
ollama pull llama3.1:70b
```

**Recomendado**: Usa `llama3.2` para desarrollo, es mÃ¡s rÃ¡pido.

## ğŸ”§ **Paso 3: Instalar dependencias del backend**

```bash
cd api-backend
npm install
```

## â–¶ï¸ **Paso 4: Iniciar los servicios**

### Terminal 1 - Ollama (si no se iniciÃ³ automÃ¡ticamente):
```bash
ollama serve
```

### Terminal 2 - Backend:
```bash
cd api-backend
npm run dev
```

### Terminal 3 - Frontend:
```bash
cd BOLT
npm run dev
```

## âœ… **Verificar que funciona**

1. Ve a http://localhost:3001/api/ai/status
2. DeberÃ­as ver:
```json
{
  "isReady": true,
  "modelName": "llama3.2",
  "service": "Ollama + Llama 3"
}
```

3. Ve al editor de clases en http://localhost:5173/editor
4. En el **Paso 3**, verÃ¡s el panel **"Asistente Inteligente con IA"**
5. Al seleccionar un template, la IA deberÃ­a generar recomendaciones reales

## ğŸ› **Troubleshooting**

### Error: "No se pudo conectar a Ollama"
- Verifica que Ollama estÃ© corriendo: `ollama --version`
- Reinicia el servicio: `ollama serve`

### Error: "Model not found"
- Verifica modelos instalados: `ollama list`
- Reinstala el modelo: `ollama pull llama3.2`

### La IA estÃ¡ muy lenta
- Usa un modelo mÃ¡s pequeÃ±o: `llama3.2` en lugar de `llama3.1:8b`
- Verifica RAM disponible: necesitas al menos 4GB libres

### Las recomendaciones siguen siendo genÃ©ricas
- Verifica en la consola del backend si hay logs de `ğŸ¤– Generando recomendaciones con IA...`
- Si no aparecen, la IA no estÃ¡ conectada correctamente

## ğŸ’¡ **Â¿QuÃ© hace la IA ahora?**

**Antes**: Solo reglas simuladas
**Ahora**: Llama 3 real que:

- âœ… **Analiza el contexto pedagÃ³gico** del template y contenidos actuales
- âœ… **Razona sobre quÃ© tipos de contenido** son mÃ¡s efectivos educativamente
- âœ… **Considera el dispositivo** (mÃ³vil vs desktop) para optimizaciÃ³n
- âœ… **Genera explicaciones personalizadas** para cada recomendaciÃ³n
- âœ… **Se adapta al tema** (matemÃ¡ticas, ciencias, historia, etc.)
- âœ… **Propone alternativas** cuando la primera opciÃ³n no es Ã³ptima

## ğŸ¯ **Ejemplo de diferencia:**

### Antes (simulado):
```
"reasoning": "[Modo bÃ¡sico] Slot requerido necesita contenido"
```

### Ahora (IA real):
```
"reasoning": "Para una introducciÃ³n de matemÃ¡ticas, un texto explicativo inicial ayuda a establecer el contexto antes de mostrar elementos visuales complejos. El contenido textual es Ã³ptimo para dispositivos mÃ³viles y permite a los estudiantes leer a su ritmo."
```

## ğŸ”„ **Sistema de Fallback**

Si Ollama no estÃ¡ disponible, el sistema automÃ¡ticamente usa el algoritmo de respaldo basado en reglas, asÃ­ que **la app siempre funciona**.

La IA se indica con:
- ğŸ¤– **"IA Activa"** - Llama 3 funcionando
- ğŸ’¤ **"IA Desactivada"** - Usando fallback

---

Â¡Ahora tienes IA **real** generando recomendaciones pedagÃ³gicas inteligentes! ğŸ‰ 